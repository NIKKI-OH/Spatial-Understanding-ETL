import json
import os
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from datasets import load_dataset
from PIL import Image

# ==========================================
# 1. è¾…åŠ©å‡½æ•°ï¼šåæ ‡è½¬æ¢ä¸å½’ä¸€åŒ–
# ==========================================
def normalize_bbox(coco_bbox, width, height):
    """
    å°† COCO æ ¼å¼ [x, y, w, h] è½¬æ¢ä¸º å½’ä¸€åŒ– [x_min, y_min, x_max, y_max]
    èŒƒå›´ 0.0 - 1.0
    """
    x, y, w, h = coco_bbox
    
    # é˜²æ­¢è¶Šç•Œ (æœ‰äº›æ•°æ®é›†æ ‡æ³¨å¯èƒ½ä¼šè¶…å‡ºå›¾ç‰‡è¾¹ç¼˜)
    x_min = max(0, x) / width
    y_min = max(0, y) / height
    x_max = min(width, x + w) / width
    y_max = min(height, y + h) / height
    
    # ä¿ç•™4ä½å°æ•°ï¼ŒèŠ‚çœç©ºé—´ä¸”è¶³å¤Ÿç²¾ç¡®
    return [round(v, 4) for v in [x_min, y_min, x_max, y_max]]

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šETL (Extract, Transform, Load)
# ==========================================
def run_etl_pipeline(num_samples=5):
    print(f"ğŸš€ å¼€å§‹æµå¼è¯»å– Visual Genome æ•°æ® (åªå–å‰ {num_samples} æ¡)...")
    
    # ä½¿ç”¨ streaming=Trueï¼Œæ— éœ€ä¸‹è½½æ•´ä¸ªæ•°æ®é›†ï¼Œç§’çº§å¯åŠ¨
    # region_descriptions_v1.2.0 åŒ…å«å›¾ç‰‡åŒºåŸŸæè¿°å’Œ bbox
    dataset = load_dataset("visual_genome", "region_descriptions_v1.2.0", split="train", streaming=True)
    
    unified_data_list = []
    
    # è¿™é‡Œçš„ iterator ä¼šä»ç½‘ç»œæµå¼è·å–æ•°æ®
    for i, item in enumerate(dataset):
        if i >= num_samples:
            break
            
        # åŸå§‹æ•°æ®æå–
        # Visual Genome çš„ HF æ ¼å¼é€šå¸¸åŒ…å«: image (PILå¯¹è±¡), regions (åˆ—è¡¨)
        image = item['image']
        width, height = image.size
        regions = item['regions'] # è¿™æ˜¯ä¸€ä¸ª listï¼Œé‡Œé¢æœ‰å¾ˆå¤šä¸ª bbox å’Œ phrase
        
        # --- æ„å»ºç»Ÿä¸€ Schema ---
        sample_entry = {
            "id": f"vg_{item['image_id']}",
            "data_source": "visual_genome",
            "task_type": "spatial_understanding",
            "media": {
                "image_size": [width, height],
                "image_path": f"virtual_path/vg_{item['image_id']}.jpg" # æ¨¡æ‹Ÿè·¯å¾„
            },
            "spatial_annotations": [],
            "conversations": []
        }
        
        # å¤„ç†è¯¥å›¾ç‰‡å†…çš„æ‰€æœ‰æ ‡æ³¨åŒºåŸŸ (è¿™é‡Œåªå–å‰3ä¸ªåšæ¼”ç¤ºï¼Œé¿å…è¿‡é•¿)
        for region in regions[:3]:
            # åŸå§‹ bbox æ˜¯ [x, y, w, h]
            raw_bbox = [region['x'], region['y'], region['width'], region['height']]
            norm_bbox = normalize_bbox(raw_bbox, width, height)
            phrase = region['phrase']
            
            # å¡«å…… Annotation
            sample_entry["spatial_annotations"].append({
                "label": "region_description",
                "bbox_2d": norm_bbox,
                "text": phrase
            })
            
            # å¡«å…… Conversation (æ„é€ æŒ‡ä»¤å¾®è°ƒæ ¼å¼)
            # æ¨¡æ‹Ÿç”¨æˆ·é—®ï¼šè¿™ä¸ªåŒºåŸŸæ˜¯ä»€ä¹ˆï¼Ÿ
            # æ¨¡æ‹Ÿ AI ç­”ï¼šæè¿° + åæ ‡
            sample_entry["conversations"].append({
                "from": "human",
                "value": f"Describe the region at <box>{norm_bbox}</box>."
            })
            sample_entry["conversations"].append({
                "from": "gpt",
                "value": phrase
            })
            
        unified_data_list.append(sample_entry)
        
        # --- 3. å®æ—¶å¯è§†åŒ–éªŒè¯ (åªç”»ç¬¬ä¸€å¼ å›¾åšè¯æ˜) ---
        if i == 0:
            visualize_verification(image, sample_entry)

    # ä¿å­˜ç»“æœ
    output_file = "unified_spatial_data.jsonl"
    with open(output_file, "w", encoding="utf-8") as f:
        for entry in unified_data_list:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
            
    print(f"âœ… å¤„ç†å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ {output_file}")
    print("ä½ å¯ä»¥æ‰“å¼€è¿™ä¸ªæ–‡ä»¶æŸ¥çœ‹æ ¼å¼ï¼Œæˆ–è€…å±•ç¤ºç”Ÿæˆçš„ 'verification_plot.png' ç»™é¢è¯•å®˜ã€‚")

# ==========================================
# 3. å¯è§†åŒ–æ¨¡å— (Proof of Work)
# ==========================================
def visualize_verification(image, schema_entry):
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–éªŒè¯å›¾...")
    plt.figure(figsize=(10, 8))
    plt.imshow(image)
    ax = plt.gca()
    
    img_w, img_h = image.size
    
    # ä»æˆ‘ä»¬è½¬åŒ–å¥½çš„ Schema é‡Œè¯»æ•°æ®ï¼Œåå‘ç”»å›å»ï¼Œè¯æ˜è½¬åŒ–æ— è¯¯
    for ann in schema_entry["spatial_annotations"]:
        # æ‹¿åˆ°å½’ä¸€åŒ–åæ ‡ [x1, y1, x2, y2]
        nx1, ny1, nx2, ny2 = ann["bbox_2d"]
        
        # åå½’ä¸€åŒ–å›åƒç´ åæ ‡
        x = nx1 * img_w
        y = ny1 * img_h
        w = (nx2 - nx1) * img_w
        h = (ny2 - ny1) * img_h
        
        # ç”»æ¡†
        rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')
        ax.add_patch(rect)
        
        # å†™å­— (é˜²æ­¢é®æŒ¡ï¼Œå†™åœ¨æ¡†ä¸Šæ–¹)
        plt.text(x, y - 5, ann["text"], color='white', fontsize=10, 
                 bbox=dict(facecolor='red', alpha=0.5))
        
    plt.axis('off')
    plt.title(f"Verification: {schema_entry['id']} (Normalized BBoxes restored)")
    plt.savefig("verification_plot.png")
    plt.show()

if __name__ == "__main__":
    run_etl_pipeline(num_samples=5)
